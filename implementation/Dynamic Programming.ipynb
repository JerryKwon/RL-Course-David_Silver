{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DP를 활용하여 RL을 수행하는 방식은, RL이 학습되는 공간에 대해 전이확률, 보상 등의 모든 정보가 주어진 MDP 환경이다.\n",
    "\n",
    "DP를 구현하기 위해서는 RL과정을 bellman equation에 따라 크게 두 가지로 나누는데,\n",
    "\n",
    "* action을 취함으로써 얻게 되는 현재의 보상\n",
    "* action 이후의 state들로 얻게되는 보상의 할인된 가치\n",
    "\n",
    "이렇게 나뉘는 구조는 DP로 구현하기에 용이하기 때문에 DP를 활용한다.\n",
    "\n",
    "DP를 활용한 방식에는 Policy Iteration 방식과 Value Iteration 방식이 있다.\n",
    "\n",
    "## Policy Iteration\n",
    "\n",
    "<img src=\"../lectures/imgs/ch3_7.jpg\"/>\n",
    "\n",
    "Policy Iteration은 Policy Evaluation과 Policy Improvement 두 가지 절차로 진행된다.\n",
    "\n",
    "Policy Evaluation은 가장 초기에 주어지는 policy로 각 state에 대해 최적의 value function을 찾아 평가하는 과정이다.\n",
    "\n",
    "Policy Improvement는 Evaluation으로 찾은 최적의 value function 값으로 다음 state로 greedy하게 진행하여 policy를 update하는 과정이다.\n",
    "\n",
    "이 두 과정을 반복하여 Optimal Value와 Optimal Policy를 찾아낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(state, action):\n",
    "    \n",
    "    # 상, 하, 좌, 우\n",
    "    action_grid = [(-1,0),(1,0),(0,-1),(0,1)]\n",
    "    \n",
    "    state[0] += action_grid[action][0]\n",
    "    state[1] += action_grid[action][1]\n",
    "    \n",
    "    # bouce back to edge\n",
    "    if state[0] < 0:\n",
    "        state[0] = 0\n",
    "    elif state[0] > 3:\n",
    "        state[0] = 3\n",
    "        \n",
    "    if state[1] < 0:\n",
    "        state[1] = 0\n",
    "    elif state[1] > 3:\n",
    "        state[1] = 3\n",
    "        \n",
    "    return state[0], state[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_width = 4\n",
    "grid_height = grid_width\n",
    "action = [0,1,2,3]\n",
    "# why add one more space for action?\n",
    "policy = np.empty([grid_height, grid_width, len(action)], dtype=float)\n",
    "\n",
    "for i in range(grid_height):\n",
    "    for j in range(grid_width):\n",
    "        for k in range(len(action)):\n",
    "            if i == j and ((i==0) or (i==3)):\n",
    "                policy[i][j] = 0.00\n",
    "            else:\n",
    "                policy[i][j] = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  , 0.  , 0.  , 0.  ],\n",
       "        [0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25]],\n",
       "\n",
       "       [[0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25]],\n",
       "\n",
       "       [[0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25]],\n",
       "\n",
       "       [[0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25],\n",
       "        [0.  , 0.  , 0.  , 0.  ]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = 10\n",
    "verbose = True\n",
    "reward = -1\n",
    "dis = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:0 \n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "iteration:1 \n",
      "[[ 0.    -1.675 -1.9   -1.9  ]\n",
      " [-1.675 -1.9   -1.9   -1.9  ]\n",
      " [-1.9   -1.9   -1.9   -1.675]\n",
      " [-1.9   -1.9   -1.675  0.   ]]\n",
      "\n",
      "iteration:2 \n",
      "[[ 0.    -2.232 -2.659 -2.71 ]\n",
      " [-2.232 -2.609 -2.71  -2.659]\n",
      " [-2.659 -2.71  -2.609 -2.232]\n",
      " [-2.71  -2.659 -2.232  0.   ]]\n",
      "\n",
      "iteration:3 \n",
      "[[ 0.    -2.688 -3.32  -3.416]\n",
      " [-2.688 -3.224 -3.371 -3.32 ]\n",
      " [-3.32  -3.371 -3.224 -2.688]\n",
      " [-3.416 -3.32  -2.688  0.   ]]\n",
      "\n",
      "iteration:4 \n",
      "[[ 0.    -3.077 -3.879 -4.031]\n",
      " [-3.077 -3.727 -3.945 -3.879]\n",
      " [-3.879 -3.945 -3.727 -3.077]\n",
      " [-4.031 -3.879 -3.077  0.   ]]\n",
      "\n",
      "iteration:5 \n",
      "[[ 0.    -3.404 -4.36  -4.56 ]\n",
      " [-3.404 -4.16  -4.423 -4.36 ]\n",
      " [-4.36  -4.423 -4.16  -3.404]\n",
      " [-4.56  -4.36  -3.404  0.   ]]\n",
      "\n",
      "iteration:6 \n",
      "[[ 0.    -3.683 -4.768 -5.014]\n",
      " [-3.683 -4.522 -4.834 -4.768]\n",
      " [-4.768 -4.834 -4.522 -3.683]\n",
      " [-5.014 -4.768 -3.683  0.   ]]\n",
      "\n",
      "iteration:7 \n",
      "[[ 0.    -3.919 -5.117 -5.402]\n",
      " [-3.919 -4.833 -5.18  -5.117]\n",
      " [-5.117 -5.18  -4.833 -3.919]\n",
      " [-5.402 -5.117 -3.919  0.   ]]\n",
      "\n",
      "iteration:8 \n",
      "[[ 0.    -4.121 -5.414 -5.734]\n",
      " [-4.121 -5.095 -5.478 -5.414]\n",
      " [-5.414 -5.477 -5.095 -4.121]\n",
      " [-5.734 -5.414 -4.121  0.   ]]\n",
      "\n",
      "iteration:9 \n",
      "[[ 0.    -4.292 -5.668 -6.017]\n",
      " [-4.292 -5.319 -5.729 -5.668]\n",
      " [-5.668 -5.729 -5.319 -4.292]\n",
      " [-6.017 -5.668 -4.292  0.   ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## poilcy evaluation\n",
    "\n",
    "# value = policy_evaluation(grid_width, grid_height, action, policy, 100)\n",
    "\n",
    "post_value_table = np.zeros([grid_height, grid_width],dtype=float)\n",
    "\n",
    "if iter_num == 0:\n",
    "    print(\"result is post_value_table\")\n",
    "    \n",
    "else:\n",
    "    for iteration in (range(iter_num)):\n",
    "        next_value_table = np.zeros([grid_height, grid_width],dtype=float)\n",
    "        for i in range(grid_height):\n",
    "            for j in range(grid_width):\n",
    "                if i==j and ((i==0) or (i==3)):\n",
    "                    value_t = 0\n",
    "                else:\n",
    "                    value_t = 0\n",
    "                    for act in action:\n",
    "                        i_, j_ = get_state([i,j], act)\n",
    "                        value = policy[i][j][act] * (reward + dis*post_value_table[i_][j_])\n",
    "                        value_t += value\n",
    "                \n",
    "                next_value_table[i][j] = round(value_t, 3)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"iteration:{} \\n{}\\n\".format(iteration, next_value_table))\n",
    "            \n",
    "        post_value_table = next_value_table                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = post_value_table\n",
    "\n",
    "## policy improvement\n",
    "\n",
    "action_match = ['Up', 'Down', 'Left', 'Right']\n",
    "action_table = []\n",
    "\n",
    "for i in range(grid_height):\n",
    "    for j in range(grid_width):\n",
    "        q_func_list = []\n",
    "        if i == j and ((i==0) or (i==3)):\n",
    "            action_table.append(\"T\")\n",
    "        else:\n",
    "            for k in range(len(action)):\n",
    "                i_, j_ = get_state([i,j],k)\n",
    "                q_func_list.append(value[i_][j_])\n",
    "            max_actions = [action_v for action_v, x in enumerate(q_func_list) if x == max(q_func_list)] \n",
    "            \n",
    "            policy[i][j] = [0] * len(action)\n",
    "            for y in max_actions:\n",
    "                policy[i][j][y] = (1/len(max_actions))\n",
    "            \n",
    "            idx = np.argmax(policy[i][j])\n",
    "            action_table.append(action_match[idx])\n",
    "            \n",
    "#     break\n",
    "action_table=np.asarray(action_table).reshape((grid_height, grid_width))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['T', 'Left', 'Left', 'Down'],\n",
       "       ['Up', 'Up', 'Down', 'Down'],\n",
       "       ['Up', 'Up', 'Down', 'Down'],\n",
       "       ['Up', 'Right', 'Right', 'T']], dtype='<U5')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "<img src=\"../lectures/imgs/ch3_10.jpg\"/>\n",
    "\n",
    "Value Iteration은 두 가지 절차로 진행되는 Policy Iteration과 달리 한 번 각 state에 대해 optimal 한 move를 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:0 \n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "iteration:0 \n",
      "[['T', 'Up', 'Up', 'Up'], ['Up', 'Up', 'Up', 'Up'], ['Up', 'Up', 'Up', 'Up'], ['Up', 'Up', 'Up', 'T']]\n",
      "\n",
      "iteration:1 \n",
      "[[ 0.  -1.  -1.9 -1.9]\n",
      " [-1.  -1.9 -1.9 -1.9]\n",
      " [-1.9 -1.9 -1.9 -1. ]\n",
      " [-1.9 -1.9 -1.   0. ]]\n",
      "\n",
      "iteration:1 \n",
      "[['T', 'Left', 'Up', 'Up'], ['Up', 'Up', 'Up', 'Up'], ['Up', 'Up', 'Up', 'Down'], ['Up', 'Up', 'Right', 'T']]\n",
      "\n",
      "iteration:2 \n",
      "[[ 0.   -1.   -1.9  -2.71]\n",
      " [-1.   -1.9  -2.71 -1.9 ]\n",
      " [-1.9  -2.71 -1.9  -1.  ]\n",
      " [-2.71 -1.9  -1.    0.  ]]\n",
      "\n",
      "iteration:2 \n",
      "[['T', 'Left', 'Left', 'Up'], ['Up', 'Up', 'Up', 'Down'], ['Up', 'Up', 'Down', 'Down'], ['Up', 'Right', 'Right', 'T']]\n",
      "\n",
      "iteration:3 \n",
      "[[ 0.   -1.   -1.9  -2.71]\n",
      " [-1.   -1.9  -2.71 -1.9 ]\n",
      " [-1.9  -2.71 -1.9  -1.  ]\n",
      " [-2.71 -1.9  -1.    0.  ]]\n",
      "\n",
      "iteration:3 \n",
      "[['T', 'Left', 'Left', 'Down'], ['Up', 'Up', 'Up', 'Down'], ['Up', 'Up', 'Down', 'Down'], ['Up', 'Right', 'Right', 'T']]\n",
      "\n",
      "iteration:4 \n",
      "[[ 0.   -1.   -1.9  -2.71]\n",
      " [-1.   -1.9  -2.71 -1.9 ]\n",
      " [-1.9  -2.71 -1.9  -1.  ]\n",
      " [-2.71 -1.9  -1.    0.  ]]\n",
      "\n",
      "iteration:4 \n",
      "[['T', 'Left', 'Left', 'Down'], ['Up', 'Up', 'Up', 'Down'], ['Up', 'Up', 'Down', 'Down'], ['Up', 'Right', 'Right', 'T']]\n",
      "\n",
      "iteration:5 \n",
      "[[ 0.   -1.   -1.9  -2.71]\n",
      " [-1.   -1.9  -2.71 -1.9 ]\n",
      " [-1.9  -2.71 -1.9  -1.  ]\n",
      " [-2.71 -1.9  -1.    0.  ]]\n",
      "\n",
      "iteration:5 \n",
      "[['T', 'Left', 'Left', 'Down'], ['Up', 'Up', 'Up', 'Down'], ['Up', 'Up', 'Down', 'Down'], ['Up', 'Right', 'Right', 'T']]\n",
      "\n",
      "iteration:6 \n",
      "[[ 0.   -1.   -1.9  -2.71]\n",
      " [-1.   -1.9  -2.71 -1.9 ]\n",
      " [-1.9  -2.71 -1.9  -1.  ]\n",
      " [-2.71 -1.9  -1.    0.  ]]\n",
      "\n",
      "iteration:6 \n",
      "[['T', 'Left', 'Left', 'Down'], ['Up', 'Up', 'Up', 'Down'], ['Up', 'Up', 'Down', 'Down'], ['Up', 'Right', 'Right', 'T']]\n",
      "\n",
      "iteration:7 \n",
      "[[ 0.   -1.   -1.9  -2.71]\n",
      " [-1.   -1.9  -2.71 -1.9 ]\n",
      " [-1.9  -2.71 -1.9  -1.  ]\n",
      " [-2.71 -1.9  -1.    0.  ]]\n",
      "\n",
      "iteration:7 \n",
      "[['T', 'Left', 'Left', 'Down'], ['Up', 'Up', 'Up', 'Down'], ['Up', 'Up', 'Down', 'Down'], ['Up', 'Right', 'Right', 'T']]\n",
      "\n",
      "iteration:8 \n",
      "[[ 0.   -1.   -1.9  -2.71]\n",
      " [-1.   -1.9  -2.71 -1.9 ]\n",
      " [-1.9  -2.71 -1.9  -1.  ]\n",
      " [-2.71 -1.9  -1.    0.  ]]\n",
      "\n",
      "iteration:8 \n",
      "[['T', 'Left', 'Left', 'Down'], ['Up', 'Up', 'Up', 'Down'], ['Up', 'Up', 'Down', 'Down'], ['Up', 'Right', 'Right', 'T']]\n",
      "\n",
      "iteration:9 \n",
      "[[ 0.   -1.   -1.9  -2.71]\n",
      " [-1.   -1.9  -2.71 -1.9 ]\n",
      " [-1.9  -2.71 -1.9  -1.  ]\n",
      " [-2.71 -1.9  -1.    0.  ]]\n",
      "\n",
      "iteration:9 \n",
      "[['T', 'Left', 'Left', 'Down'], ['Up', 'Up', 'Up', 'Down'], ['Up', 'Up', 'Down', 'Down'], ['Up', 'Right', 'Right', 'T']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## value evaluation\n",
    "\n",
    "iter_num = 10\n",
    "verbose = True\n",
    "reward = -1\n",
    "dis = 0.9\n",
    "\n",
    "# value = policy_evaluation(grid_width, grid_height, action, policy, 100)\n",
    "\n",
    "post_value_table = np.zeros([grid_height, grid_width],dtype=float)\n",
    "\n",
    "action_match = ['Up', 'Down', 'Left', 'Right']\n",
    "\n",
    "if iter_num == 0:\n",
    "    print(\"result is post_value_table\")\n",
    "    \n",
    "else:\n",
    "    for iteration in (range(iter_num)):\n",
    "        next_value_table = np.zeros([grid_height, grid_width],dtype=float)\n",
    "        action_table = list()\n",
    "        for i in range(grid_height):\n",
    "            action_row_table = list()\n",
    "            for j in range(grid_width):\n",
    "                if i==j and ((i==0) or (i==3)):\n",
    "                    value_t = 0\n",
    "                    action_t = \"T\"\n",
    "                else:\n",
    "                    value_t_list = list()\n",
    "                    for act in action:\n",
    "                        i_, j_ = get_state([i,j], act)\n",
    "                        value = reward + dis*post_value_table[i_][j_]\n",
    "                        value_t_list.append(value)\n",
    "                \n",
    "                    value_t = max(value_t_list)\n",
    "                    action_t = action_match[np.argmax(value_t_list)]\n",
    "                \n",
    "                action_row_table.append(action_t)\n",
    "                next_value_table[i][j] = value_t \n",
    "            \n",
    "            action_table.append(action_row_table)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"iteration:{} \\n{}\\n\".format(iteration, next_value_table))\n",
    "            print(\"iteration:{} \\n{}\\n\".format(iteration, action_table))\n",
    "        \n",
    "        \n",
    "        post_value_table = next_value_table                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
