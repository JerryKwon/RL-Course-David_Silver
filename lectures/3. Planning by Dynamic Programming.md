# 3. Planning by Dynamic Programming

Dynamic Programming을 통해 MDP('Planning')를 해결하는 Process에 대해서 알아본다.

## reference

https://dnddnjs.gitbooks.io/rl/

https://mynsng.github.io/reinforcement%20learning/2020/02/15/DSRL-03/

https://sumniya.tistory.com/10


## 3.1. Introduction

Dynamic Programming
* Dynamic: 임의의 문제를 sequential하게 해결
* (Mathematical) Programming: 문제를 최적화하는 행위 i.e. policy in MDP

Dynamic Programming 문제를 해결하는 것은 하나의 문제를 하위 문제로 지속적으로 나누어 해결하고 결합하는 것이다. (divide and conquer)

MDP를 해결하기 위해 사용한 bellman equation은 현재의 reward와 현재가치로 할인한 미래 시점의 bellman 방정식의 합으로 나타낼 수 있기 때문에 DP 방식으로 문제에 접근할 수 있다.

<img src="imgs/ch2_15.jpg"/>

<img src="imgs/ch3_1.jpg"/>

DP 방식은 MDP에 대해 모든 것을 알고 있는 상태(보상과 전이확률)를 가정하고 사용한다.

각 state 별로 policy를 따르는 value값을 예측하기 위한 **prediction 절차**와(policy의 optimal value 계산),

이를 바탕으로, 최적의 policy를 구하는 **control 절차**로 구성된다(policy optimal value로 새로운 policy 계산).

## 3.2. Policy Evaluation (Prediction) 

-- policy가 정해져있을 때, 이 policy를 따라가면 value function이 어떻게 되어있을지 학습하는 것(objective: finding policy).

주어진 policy(pi)를 평가하기 위한 방법으로는, Bellman expectation 방정식을 loop를 통해 계산한다.

<img src="https://latex.codecogs.com/svg.latex?v_{\pi}(s)=E_{\pi}[R_{t+1}+\gamma{v_{\pi}(S_{t+1})|S_t=s}]" title="v_{\pi}(s)=E_{\pi}[R_{t+1}+\gamma{v_{\pi}(S_{t+1})|S_t=s}]" />

<img src="https://latex.codecogs.com/svg.latex?q_{\pi}(s,a)=E_{\pi}[R_{t+1}+\gamma{q_{\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a}]" title="q_{\pi}(s,a)=E_{\pi}[R_{t+1}+\gamma{q_{\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a}]" />

<img src="imgs/ch3_2.jpg"/>

초기값으로 value값을 0으로 모두 세팅하고, iteration을 통해서 policy를 따랐을 때의 value function을 찾는 것을 일컫는다. 

**backup(==memory)**

Synchronous backup, 모든 state들에 대해서 한번씩 update를 진행하는 방법. 현재 state의 value를 update하기 위해 다음 state값을 활용하여 현재 iteration의 현재 state의 value값을 update한다. 

<img src="imgs/ch3_3.jpg"/>

특정 state에서 도달 할 수 있는 state들의 value 값을 가지고 현재 state의 value 값을 iterative하게 변경하는 것.

### Example: Small Gridworld

<img src="imgs/ch3_4.jpg"/>

<img src="imgs/ch3_5.jpg"/>

<img src="imgs/ch3_6.jpg"/>

grid world example에 대해 1행 2열의 값 변동에 대해서만 자세히 알아보자.

k=0, 모든 value function의 값은 0으로 setting해둔다.

k=1, (1,2)포인트에서 이동할 때 발생하는 reward는 1이고 v0는 모든 점에서 0임을 반영하면,

<img src="https://latex.codecogs.com/svg.latex?4*\{(1/4)*(-1+\gamma{P^{\pi}}*0)\}=-1" title="4*\{(1/4)*(-1+\gamma{P^{\pi}}*0)\}=-1" />

k=2, k는 1일때와 달리 v1부터는 0이 아닌 값을 가지기 떄문에 할인하여 계산하는 부분(but, gamma=-1)을 계산해 주어야 한다.

상하좌우(1/4)로 이동시 벽에 부딪히는 경우는 bounce back한다.

<img src="https://latex.codecogs.com/svg.latex?1\times{0.25(-1+0)}+3\times{0.25(-1+(-1))}=-1.7" title="1\times{0.25(-1+0)}+3\times{0.25(-1+(-1))}=-1.7" />

도착한 state의 value값을 할인하고 reward를 더하여 policy를 반영한 값들을 summation.

점점 특정 point에 대해 value function update를 위한 state의 수를 확장하게 되면 random policy에 대한 true function을 구할 수 있게 된다.(![formular](https://render.githubusercontent.com/render/math?math=V^\pi))

Saying by Silver.

바보같은 Random Policy(0.25 per direction)를 evaluation만 했고, 평가된 Policy를 greedy 하게 움직이는 것 만으로도 Optimal policy에 도달 할 수 있다.

## 3.3. Policy Iteration (Control)

1. policy를 평가 - value function을 찾고.
2. policy를 improvement - value function을 greedy하게 가져가는 방향으로 policy를 수정.

이 둘을 반복하게 되면 최적 policy인 pi-star에 근접하게 될 것이다. 

Policy Evaluation은 특정 MDP가 가지고 있는 Policy를 가지고 value function을 update하는 것이다.

Policy Iteration은 update한 value function(![formular](https://render.githubusercontent.com/render/math?math=V^\pi))에 대해 greedy한 방식으로 행동하여 Policy를 향상시키는 것이다.

<img src="imgs/ch3_7.jpg"/>

greedy하게 policy를 향상시키는 방식은 다음 state의 value function이 큰 곳으로 향하도록 action을 취하는 것이다.

<img src="imgs/ch3_8.jpg"/>

증명하고자 하는것, Policy Improvement한 방식으로 하면 이전 policy보다 더 나아지는가?

policy -> deterministic(greedy) way policy.
q에 대해 greedy 하게 움직이게 되면 pi-prime은 기존의 pi보다 낫다는 것을 증명하고자 한다. 

## 3.4. Value Iteration

Policy Iteration과 달리 Bellman Optimal Equation을 활용한다.

<img src="imgs/ch3_9.jpg"/>

<img src="imgs/ch3_10.jpg"/>

value iteration에서 policy improvement와 같은 과정이 없는 이유는 현재 policy가 optimal하다는 것을 전제하여 value func를 max하기 때문이다.