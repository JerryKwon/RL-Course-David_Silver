# Model-Free Control

about finding policy

1. Introduction RL
2. MDP
3. Dynamic Programming when env knows.
4. unlike ch3, prediction -> policy를 가지고 value가 어떻게될지
5. 최적의 policy를 찾는것(without no info about env).

==================== 강화학습의 흐름

On-policy vs Off-policy

* On-policy: 학습하고자하는 정책과 실제 env로 경험을 쌓는 정책이 동일할 때.

학습 - table을 만들어두고 빈칸을 채우는 형태로.
if -> state가 많게 되면 이를 모두 처리할 수 없기 때문에 function approximator라는 개념이 등장(in chap 6,7).
(수많은 state에 대해 대응)

Two policies
1. Target policy to be optimized
2. behavior policy for getting experience in environment

* On-policy: 학습하고자하는 정책과 실제 env로 경험을 쌓는 정책이 동일할 때. (i.e. Alpha Go)
* Off-policy: 학습하고자하는 정책과 실제 env로 경험을 쌓는 정책이 동일하지 않을때.
    - 다른 agent의 behavior policy로 학습.

Policy Iteration
* evaluation: 기존의 policy로 가치 function 을 계산
* improvement: 계산된 가치 function을 greedy한 방식으로 하여 policy를 갱신

why MC evalution method cannot adapt in Policy Iteration?

A. MC is Model-Free Method.

V를 가지고 greedy하게 policy를 만드는 것은, 현재 state에서 다음 state들이 가지고 있는 V값을 알고서 이를 maximize한 방향으로 policy를 선택하는 것.

그런데, 다음 state를 안다는 것은 MDP를 안다는 것과 동일하다. MDP를 모른다는 것은 다음 state가 무엇이 될지를 모른다는 것. MC는 MDP를 모르는 상태의 'Model-Free' 방식이기 때문에 Evaluation은 가능하되, Improvement를 적용할 수 없다.

그렇다면, state-value가 아닌, action-value Q에대해서 greedy 하게는 적용할 수 있을까?
using table (State X Action unlike V[State X State])

A. 할 수 있다. 왜냐하면 특정 state에서 어떤 action을 취할 수 있는지는 Model-Free 방법이라도 알 수 있기 때문.

"STOCK when doing greedy; lack of exploration"

So, e-Greedy Exploration
* e로 인해 모든 case에 Exploration
* 1-e로 인해 policy를 발전시킬수 있음

slide 14.
e-greedy로도 Policy Improvement가 이루어질 수 있는지?

