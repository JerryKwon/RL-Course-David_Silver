# 1. Introduction to Reinforcement Learning

## 1.2. About Reinforcement Learning

### Characteristics of Reinforcement Learning

RL이 다른 ML 방법과 다른 이유?
* 좋고 나쁨을 판단할 supervisor가 없으며, reward값 만을 활용한다.
* sequential decision에 대하여 즉각적으로 feedback을 진행하지 않는다.
* 시간적인 요소가 작용한다(특정 episode에 대하여 순서가 중요하다.).
* agent의 action이 추후에 받을 state에 영향을 준다.

## 1.3. The Reinforcement Learning Problem

### Reward
reward는 agent의 action(step)에 대한 scalar feedback이다. agent는 누적 reward를 최대화하려고 한다.

i.e. 문제가 너무 복잡해서 reward를 벡터로 줄 수 밖에 없는 경우에는 다른 방식으로 문제를 해결해야 한다.

다시 말해, RL은 Reward Hypotheis을 기반으로 이루어진다고 할 수 있다.

* Def. Reward Hypotheis

    All goals can be described by the maximisation of expected cumulative reward

    강화학습 문제에 대한 모든 목적은 누적 보상의 기대값을 최대화하는것으로 묘사할 수 있다.

#### Sequential Decision Making (Objective of RL)

* Goal: total future reward를 최대화하는 action을 선택하는 것
* Action의 결과는 장기적으로 나타난다.
* Reward는 지연될 수 있다.
* 때때로는, 미래의 long-term reward를 위해 현재의 reward를 희생할 수 있다(not greedy consistantly).

### Environment

<div>
<img src="imgs/ch1_1.jpg"/>
</div>

agent의 action(step-행동의 단위)은 environment로 부터만 영향을 받으며, environment는 agent의 action을 받아 그에 해당하는 observation과 reward를 반환한다.

### History and State

history는 아래와 같이 observation, action, reward element의 순서로 이루어진 결과값이다.

<div>
<img src="imgs/ch1_2.jpg"/>
</div>

따라서, 이 history를 기반으로,

* agent가 action을 선택하고,
* environment가 observation과 reward를 반환한다.

State(Function of History - History에 있는 정보를 가공하여 가져오는 것.)는 다음으로 어떤 일이 일어날지를 결정하는 정보이며, 아래와 같이 나타낼 수 있다(Important than history).

<div>
<img src="imgs/ch1_3.jpg"/>
</div>

* Environment State ![formular](https://render.githubusercontent.com/render/math?math=S^e_t)

    Environment만 알고있는 정보로써, agent에는 blackbox의 영역이다.

* Agent State ![formular](https://render.githubusercontent.com/render/math?math=S^a_t)

    Agent의 action(step)에 의해 Environment가 반응한 observaion과 reward를 임의의 agent function f를 거친 결과를 일컫는다.

    <div>
    <img src="imgs/ch1_4.jpg"/>
    </div>    

* Informational State(Markov state)

    <div>
    <img src="imgs/ch1_5.jpg"/>
    </div>    

    next state(t+1)은 previous state(t)의 영향을 받으며, previous state는 더 이전의 state들에 영향을 받으므로 위와 같이 나타낼 수 있다.

* Fully Observable Environment (Markov Decision Process; MDP)

    Agent state = environment state = information state

* Partially Observable Environment (Partially Observable Markov Decision Process; POMDP)

    Agent state ![formular](https://render.githubusercontent.com/render/math?math=\neq) enviornment state

    i.e) 포커에서 공개패를 제외한 다른 사용자의 패를 알 수 없는 상황

## 1.4. Inside An RL Agent

1. Policy: agent's behavior function

    * Deterministic
    * Stochastic

2. Value Function: how good is each state and / or action

    prediction of future reward for evaluating goodness or badness of state

    <div>
    <img src="imgs/ch1_6.jpg"/>
    </div>    


3. Model: Agent's representation of the environment

    environment에 action을 가할때 어떤 Transition과 reward를 얻을지 예측하는 predicator

    <div>
    <img src="imgs/ch1_7.jpg"/>
    </div>    

4. Categorizing RL Agent

    <div>
    <img src="imgs/ch1_8.jpg"/>
    </div>    


    * Value Based
        * Value Function 
    * Policy Based
        * Policy
    * Actor Critic
        * Policy
        * Value Function
    * Model Free
        * Policy and / or Value Function 
    * Model Based
        * Policy and / or Value Function 
        * Model  

5. RL and Planning for Sequential Decision Making

    * RL
        * unknown environment
        * interact with env
        * agent improve policy

    * Planning
        
        Agent already knows about environment of problem. 

6. Exploration vs Exploitation

    * Exploration: find more information about env

    * Exploitation: exploit known infomation to maximize reward

7. Predction vs Control

    * Prediction: evaluate the future

    * Control: optimize the future
